{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PJ26487/Stock_Market_Analysis/blob/main/Sentiment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5SaEUSaVbSl",
        "outputId": "9acdbcdc-a544-4b75-c659-39158ff4d3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting finnhub-python\n",
            "  Downloading finnhub_python-2.4.13-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from finnhub-python) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "# installing libraries \n",
        "! pip install finnhub-python\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gH4HJxyVVP0"
      },
      "outputs": [],
      "source": [
        "# importing all the libraries \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import time \n",
        "\n",
        "# importing spacy and its libraries \n",
        "import spacy \n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_md\n",
        "\n",
        "import json \n",
        "import itertools\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import pprint \n",
        "import requests\n",
        "\n",
        "# sentiment analysis library\n",
        "import xgboost as xgb\n",
        "import textblob \n",
        "from textblob import TextBlob\n",
        "\n",
        "# plotting graphs library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk \n",
        "import nltk.corpus\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import finnhub\n",
        "import xgboost\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "from datetime import timedelta\n",
        "import dateutil.relativedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rktF5B3ZZ8EV",
        "outputId": "9c6b2b13-e74d-4f8f-821b-2daae35f181f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Timestamp('2022-06-28 00:00:00')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# defining current time elements\n",
        "nlp = en_core_web_md.load()\n",
        "q = 'TSLA'\n",
        "from_time = '2021-08-01' \n",
        "to_time = '2022-07-20'\n",
        "current_month = datetime.now().month\n",
        "current_year = datetime.now().year\n",
        "current_day = datetime.now().day\n",
        "current_date = str(current_year)+'-'+str(current_month)+'-'+str(current_day)\n",
        "current_date = pd.to_datetime(current_date)\n",
        "last_month_date = current_date - dateutil.relativedelta.relativedelta(months=1)\n",
        "last_month_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srgrBBkfpHCy"
      },
      "outputs": [],
      "source": [
        "month_list = [current_date,current_date-dateutil.relativedelta.relativedelta(months=1),current_date-dateutil.relativedelta.relativedelta(months=2),current_date-dateutil.relativedelta.relativedelta(months=3),current_date-dateutil.relativedelta.relativedelta(months=4)]\n",
        "month_list_1 = [current_date-dateutil.relativedelta.relativedelta(months=1),current_date-dateutil.relativedelta.relativedelta(months=2),current_date-dateutil.relativedelta.relativedelta(months=3),current_date-dateutil.relativedelta.relativedelta(months=4),current_date-dateutil.relativedelta.relativedelta(months=5)]\n",
        "\n",
        "for i in month_list:\n",
        "\n",
        "  api_key = 'cbcgogiad3ib4g5ulrb0'\n",
        "  sandbox_key = 'sandbox_cbcgogiad3ib4g5ulrbg'\n",
        "  finnhub_client = finnhub.Client(api_key=api_key)\n",
        "  response =  finnhub_client.company_news('TSLA',_from='2021-08-01',to='2022-06-25')\n",
        "  response_str = ' '.join(map(str, response))\n",
        "\n",
        "  title_list =[]\n",
        "  date_list = []\n",
        "  desc_list = []\n",
        "  sent_list = []\n",
        "  date_real_list = []\n",
        "  # sentiment will be checked with the title and the spacy library will be run on the summary\n",
        "\n",
        "  for i in response:\n",
        "    title = list(i.values())[2]\n",
        "    timestamp = list(i.values())[1]\n",
        "    summ = list(i.values())[7]\n",
        "    sent = TextBlob(title).sentiment.polarity\n",
        "    title_list.append(title)\n",
        "    date_list.append(timestamp)\n",
        "    desc_list.append(summ)\n",
        "    sent_list.append(sent)\n",
        "\n",
        "\n",
        "  for j in date_list:\n",
        "    dty_obj=dt.datetime.fromtimestamp(j).strftime('%d-%m-%y')\n",
        "    date_real_list.append(dty_obj)\n",
        "    \n",
        "\n",
        "  df_y = pd.DataFrame()\n",
        "  df_y['Title'] = title_list\n",
        "  df_y['Summary'] = desc_list\n",
        "  df_y['Sentiment'] = sent_list\n",
        "  df_y['date'] = date_real_list\n",
        "\n",
        "  # filtering the dfs into sentiment by date and setting them into another df for graphing\n",
        "  title_date_sent = df_y.groupby('date')['Sentiment'].mean()\n",
        "\n",
        "  (df_y['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YoewKA1HLa1J"
      },
      "outputs": [],
      "source": [
        "# defining functions to create a streamlit dashboard \n",
        "\n",
        "# extraction of news articles from finnhub for the time period specified\n",
        "def finnews(from_time,to_time,topic):\n",
        "  api_key = 'cbcgogiad3ib4g5ulrb0'\n",
        "  sandbox_key = 'sandbox_cbcgogiad3ib4g5ulrbg'\n",
        "  finnhub_client = finnhub.Client(api_key=api_key)\n",
        "  response =  finnhub_client.company_news(topic,_from=from_time,to=to_time)\n",
        "  response_str = ' '.join(map(str, response))\n",
        "\n",
        "  title_list =[]\n",
        "  date_list = []\n",
        "  desc_list = []\n",
        "  sent_list = []\n",
        "  date_real_list = []\n",
        "  # sentiment will be checked with the title and the spacy library will be run on the summary\n",
        "\n",
        "  for i in response:\n",
        "    title = list(i.values())[2]\n",
        "    timestamp = list(i.values())[1]\n",
        "    summ = list(i.values())[7]\n",
        "    sent = TextBlob(title).sentiment.polarity\n",
        "    title_list.append(title)\n",
        "    date_list.append(timestamp)\n",
        "    desc_list.append(summ)\n",
        "    sent_list.append(sent)\n",
        "\n",
        "\n",
        "  for j in date_list:\n",
        "    dty_obj=dt.datetime.fromtimestamp(j).strftime('%d-%m-%y')\n",
        "    date_real_list.append(dty_obj)\n",
        "    \n",
        "\n",
        "  df_y = pd.DataFrame()\n",
        "  df_y['Title'] = title_list\n",
        "  df_y['Summary'] = desc_list\n",
        "  df_y['Sentiment'] = sent_list\n",
        "  df_y['date'] = date_real_list\n",
        "\n",
        "  return df_y\n",
        "\n",
        "  # filtering the dfs into sentiment by date and setting them into another df for graphing\n",
        "  title_date_sent = df_y.groupby('date')['Sentiment'].mean()\n",
        "  return title_date_sent\n",
        " \n",
        "# extracrion of one month news from the news api\n",
        "def monthly_news(from_time,to_time,topic):\n",
        "  url = f'https://newsapi.org/v2/everything?q={q}&from={from_time}&to={to_time}&sortBy=popularity&apiKey=c4c3784a49be4de48d3da1b85a93d239'\n",
        "  response = requests.get(url)\n",
        "  # Convert the response to JSON format and pretty print it\n",
        "  response_json = response.json()\n",
        "  #pprint.pprint(response_json)\n",
        "  response_json_1 = list(response_json.values())[2]\n",
        "  response_json_2 = json.dumps(response_json_1)\n",
        "  print(response_json)\n",
        "  aList = json.loads(response_json_2)\n",
        "  title_list =[]\n",
        "  date_list = []\n",
        "  desc_list = []\n",
        "  title_sentiment_list = []\n",
        "  desc_sentiment_list = []\n",
        "  for i in range(len(aList)):\n",
        "    lola = aList[i]\n",
        "    title = list(lola.values())[2]\n",
        "    date_publish = list(lola.values())[6]\n",
        "    desc = list(lola.values())[3]\n",
        "    title_list.append(title)\n",
        "    date_list.append(date_publish)\n",
        "    desc_list.append(desc)\n",
        "  df = pd.DataFrame()\n",
        "  df['Title'] = title_list\n",
        "  df['Date'] = date_list\n",
        "  df['Description'] = desc_list\n",
        "  for i in range(len(df)):\n",
        "    #title_sentiment = df.iloc[i]['Title']\n",
        "    #desc_sentiment = df.iloc[i]['Description']\n",
        "    title_sentiment = (TextBlob(df.iloc[i]['Title']).sentiment.polarity)\n",
        "    desc_sentiment = (TextBlob(df.iloc[i]['Description']).sentiment.polarity)\n",
        "    title_sentiment_list.append(title_sentiment)\n",
        "    desc_sentiment_list.append(desc_sentiment)\n",
        "    \n",
        "  df['Title Sentiment'] = title_sentiment_list\n",
        "  df['Description Sentiment'] = desc_sentiment_list\n",
        "\n",
        "\n",
        "  # filtering the dfs into sentiment by date and setting them into another df for graphing\n",
        "  df['date'] = pd.to_datetime(df['Date']).dt.date\n",
        "  title_date_sent = df.groupby('date')['Title Sentiment'].mean()\n",
        "  desc_date_sent = df.groupby('date')['Description Sentiment'].mean()\n",
        "\n",
        "  print(title_date_sent)\n",
        "\n",
        "\n",
        "  plt.title(f'News Title Sentiment for {q}')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Sentiment Value')\n",
        "  plt.plot(title_date_sent)\n",
        "  plt.show \n",
        "\n",
        "  plt.title(f'News Sentiment for {q}')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Sentiment Value')\n",
        "  plt.plot(desc_date_sent)\n",
        "  plt.show\n",
        "\n",
        "\n",
        "# extraction of spacy library information from the df\n",
        "def spacy(news_str):\n",
        "  #remove punctuations from the string\n",
        "  punctuations = '''!()-[]\\{};:'\"/,<>./?@#$%^&*_~]1234567890''' #list of all the punctuations we remove from the list\n",
        "  news_str_mt = \"\"\n",
        "  for char in news_str:\n",
        "      if char not in punctuations:\n",
        "          news_str_mt = news_str_mt + char\n",
        "  #creating the string into a list\n",
        "  news_list_space= news_str_mt.split(\" \")\n",
        "  news_list_punc= []\n",
        "  for i in news_list_space:\n",
        "    if i is not '':\n",
        "      news_list_punc.append(i)\n",
        "  #removing the stopwords from the list\n",
        "  stop_str= \"rt RT shall \\n a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can cannot cant co computer con could couldnt cry de describe detail do done down due during each eg eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen fify fill find fire first five for former formerly forty found four from front full further get give go had has hasnt have he hence her here hereafter hereby herein hereupon hers herse him himse his how however hundred i ie if in inc indeed interest into is it its itse keep last latter latterly least less ltd made many may me meanwhile might mill mine more moreover most mostly move much must my myse name namely neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off often on once one only onto or other others otherwise our ours ourselves out over own part per perhaps please put rather re same see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under until up upon us very via was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you your yours yourself yourselves\"\n",
        "  stop_list= stop_str.split(\" \")\n",
        "  news_list_stop=[]\n",
        "  #removing the stopwords from the sentences\n",
        "  for j in news_list_punc:\n",
        "    if j not in stop_list:\n",
        "      news_list_stop.append(j)\n",
        "  #removing the stopwords from the sentences\n",
        "  news_stop_str = \" \"\n",
        "  for char in news_str_mt:\n",
        "    if char not in stop_str:\n",
        "      news_stop_str = news_stop_str + char\n",
        "  #stemming the words\n",
        "  s_stemmer = SnowballStemmer(language='english') #setting the language\n",
        "  news_stem=[]\n",
        "  for word in news_list_stop:\n",
        "      news_stem.append(s_stemmer.stem(word))\n",
        "\n",
        "  news_list_count= Counter(news_stem)\n",
        "\n",
        "  #converting the dictionary into a list \n",
        "  keys= news_list_count.keys()\n",
        "  values= news_list_count.values()\n",
        "\n",
        "  #converting the list into a df \n",
        "  tweets_df= pd.DataFrame({'count':values,'keywords': keys})\n",
        "\n",
        "  #we can order the pandas df in descending order\n",
        "  tweets_df = tweets_df.sort_values(by=['count','keywords'], ascending = False)\n",
        "  print(tweets_df.head(10))\n",
        "\n",
        "  #Making the spacy library work\n",
        "  doc = nlp(news_str)\n",
        "  print(len(doc))\n",
        "\n",
        "  label = [(X.text, X.label_) for X in doc.ents]\n",
        "\n",
        "  df_1 = pd.DataFrame(label, columns = ['word','entity'])\n",
        "\n",
        "  # getting Country values in spacy\n",
        "  df_2 = df_1.where(df_1['entity'] == 'NORP')\n",
        "  df_2 = df_2['word'].value_counts().nlargest(10)\n",
        "  print(df_2)\n",
        "  \n",
        "  # the people mentioned in the news\n",
        "  df_3 = df_1.where(df_1['entity']== 'PERSON')\n",
        "  df_3 = df_3['word'].value_counts().nlargest(10)\n",
        "  print('the people/products mentioned have been')\n",
        "  print(df_3)\n",
        "\n",
        "  # the organisations mentioned in the news\n",
        "  df_4 = df_1.where(df_1['entity']=='ORG')\n",
        "  df_4 = df_4['word'].value_counts().nlargest(10)\n",
        "  print(\"The Organisations mentioned have been \")\n",
        "  print(df_4)\n",
        "\n",
        "  df_5 = df_1.where(df_1['entity']== 'EVENT')\n",
        "  df_5 = df_5['word'].value_counts().nlargest(10)\n",
        "  print(\"The Events mentioned were\")\n",
        "  print(df_5)\n",
        "\n",
        "# extraction of stock ticker for one year \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4QHZ1DQdRArc",
        "outputId": "425082ff-51cd-4aa8-aac2-7a66ca3f3d02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                               Title  \\\n",
            "0                                      Rivian to cut headcount by 6%   \n",
            "1                                  Rivian laying off 6% of its staff   \n",
            "2                   7 Deeply Oversold Electric Vehicle Stocks to Buy   \n",
            "3                              Amazon's High-Stakes Earnings Results   \n",
            "4                     Rivian cuts workforce by 6%, Bloomberg reports   \n",
            "..                                                               ...   \n",
            "4                      Wall Street Breakfast: New Era Of Spaceflight   \n",
            "5         My 32 Stock Retirement Portfolio Breaks $400,000 In August   \n",
            "6   Wall Street Breakfast: Hurricane Ida Batters Louisiana (Podcast)   \n",
            "7                        Wall Street Breakfast: Harsh As Apple Cider   \n",
            "0      Is Lucid Motors Stock A Buy Or Sell After Recent SPAC Merger?   \n",
            "\n",
            "                                                                                                                                                                                            Summary  \\\n",
            "0   Electric-truck maker Rivian AutomotiveInc said on Wednesday it was reducing headcount by 6%in order to optimize costs in a tightening macroeconomicenvironment. Staff in the... | July 27, 2022   \n",
            "1                                                             EV-maker Rivian Automotive Incsaid on Wednesday it was laying off 6% of its staff as part of arestructuring plan. ... | July 27, 2022   \n",
            "2                                                    These are among the best oversold electric vehicle stocks to buy in Q3. CHPT, FSR, LCID, NIO, RIVN, TSLA, and XPEV can make great investments.   \n",
            "3                                                          Amazon's past quarters' challenges will likely brim over Q2 earnings this Thursday. Check out what could cause a sell-off in AMZN stock.   \n",
            "4                                                                                                                                                                 Rivian is slashing roughl... RIVN   \n",
            "..                                                                                                                                                                                              ...   \n",
            "4                                                         Listen on the go! A daily podcast of Wall Street Breakfast will be available by 8:00 a.m. on Seeking Alpha, iTunes, Stitcher and Spotify.   \n",
            "5                                         My portfolio, built specifically for my retirement, eclipses new highs. See here the stocks that I'm looking to add to my portfolio in the coming months.   \n",
            "6                          Our Top Stories Today Are: Hurricane Ida Batters Louisiana, Some United Airlines 777 Boeing Jets May Be Grounded Until Next Year and Look Out Tesla Hereâ€™s Comes Rivian.   \n",
            "7                                                         Listen on the go! A daily podcast of Wall Street Breakfast will be available by 8:00 a.m. on Seeking Alpha, iTunes, Stitcher and Spotify.   \n",
            "0                                     Lucid Motors completed its merger with CCIV, which has resulted in a strong balance sheet that should fuel future growth. See if LCID stock is a buy or sell.   \n",
            "\n",
            "    Sentiment      date  \n",
            "0    0.000000  27-07-22  \n",
            "1    0.000000  27-07-22  \n",
            "2    0.000000  27-07-22  \n",
            "3    0.000000  27-07-22  \n",
            "4    0.000000  27-07-22  \n",
            "..        ...       ...  \n",
            "4    0.136364  15-09-21  \n",
            "5    0.000000  01-09-21  \n",
            "6    0.000000  30-08-21  \n",
            "7   -0.200000  30-08-21  \n",
            "0    0.000000  05-08-21  \n",
            "\n",
            "[1843 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# setting up the test environment\n",
        "month_list = [current_date,current_date-dateutil.relativedelta.relativedelta(months=1),current_date-dateutil.relativedelta.relativedelta(months=2),current_date-dateutil.relativedelta.relativedelta(months=3),current_date-dateutil.relativedelta.relativedelta(months=4),current_date-dateutil.relativedelta.relativedelta(months=5),current_date-dateutil.relativedelta.relativedelta(months=6),current_date-dateutil.relativedelta.relativedelta(months=7),current_date-dateutil.relativedelta.relativedelta(months=8),current_date-dateutil.relativedelta.relativedelta(months=9),current_date-dateutil.relativedelta.relativedelta(months=10),current_date-dateutil.relativedelta.relativedelta(months=11)]\n",
        "month_list_1 = [current_date-dateutil.relativedelta.relativedelta(months=1),current_date-dateutil.relativedelta.relativedelta(months=2),current_date-dateutil.relativedelta.relativedelta(months=3),current_date-dateutil.relativedelta.relativedelta(months=4),current_date-dateutil.relativedelta.relativedelta(months=5),current_date-dateutil.relativedelta.relativedelta(months=6),current_date-dateutil.relativedelta.relativedelta(months=7),current_date-dateutil.relativedelta.relativedelta(months=8),current_date-dateutil.relativedelta.relativedelta(months=9),current_date-dateutil.relativedelta.relativedelta(months=10),current_date-dateutil.relativedelta.relativedelta(months=11),current_date-dateutil.relativedelta.relativedelta(months=12)]\n",
        "\n",
        "df_list_y = []\n",
        "\n",
        "for i in range(len(month_list)):\n",
        "  df_list_y.append(finnews(str(month_list_1[i])[:10],str(month_list[i])[:10],'RIVN'))\n",
        "\n",
        "df_concat = pd.concat(df_list_y)\n",
        "print((df_concat))\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U1cSiSIjzwL"
      },
      "outputs": [],
      "source": [
        "import dateutil\n",
        "df_concat = pd.concat(df_list_y)\n",
        "\n",
        "df_concat['date'] = pd.to_datetime(df_concat['date'])\n",
        "title_date_sent = df_concat.groupby('date')['Sentiment'].mean()\n",
        "title_date_sent\n",
        "\n",
        "plt.title(f'News Title Sentiment for Stock')\n",
        "plt.xlabel('date')\n",
        "plt.xlim([current_date-dateutil.relativedelta.relativedelta(months=12),current_date])\n",
        "plt.ylabel('Sentiment Value')\n",
        "plt.plot(title_date_sent)\n",
        "plt.show\n",
        "\n",
        "# spacy by default will display the news that has occured in the line of sight\n",
        "print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCsGcfiVlxQs"
      },
      "outputs": [],
      "source": [
        "# for spacy will only be run on the last month of the final requirem\n",
        "\n",
        "import yfinance as yf \n",
        "name = 'GOOGL'\n",
        "start = '2021-09-01'\n",
        "end = current_date\n",
        "stock_df = yf.download(name, \n",
        "                      start=start, \n",
        "                      end=end, \n",
        "                      progress=False)\n",
        "\n",
        "stock_df['date'] = stock_df.index \n",
        "stock_df.plot(x ='date', y ='Close',kind = 'line')\n",
        "plt.show()\n",
        "\n",
        "# using the ticker to get more information \n",
        "tsla = yf.Ticker(name)\n",
        "print('the major shareholders are')\n",
        "df_share = tsla.major_holders\n",
        "df_recom = tsla.recommendations\n",
        "df_options = tsla.options\n",
        "df_sus = tsla.sustainability\n",
        "\n",
        "start = pd.to_datetime(start)\n",
        "start = start.to_pydatetime()\n",
        "\n",
        "# filtering the dfs\n",
        "df_recom = df_recom.tail(10)\n",
        "\n",
        "#df_sus = df_sus[df_sus['']]\n",
        "print(tsla.major_holders)\n",
        "print(' ')\n",
        "print('Recommendations')\n",
        "print(df_recom)\n",
        "print(' ')\n",
        "print('Options')\n",
        "print(tsla.options)\n",
        "print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neaAeaUFzypw"
      },
      "outputs": [],
      "source": [
        "# spacy analysis of the current month \n",
        "import dateutil\n",
        "df_spacy = df_concat[df_concat['date'].dt.year == current_year]\n",
        "df_spacy = df_spacy[df_spacy['date'] >= current_date-dateutil.relativedelta.relativedelta(months=1)]\n",
        "df_spacy\n",
        "\n",
        "# converting the summary into a string \n",
        "news_list = df_spacy['Summary'].to_list()\n",
        "news_list\n",
        "news_str = ' '.join(map(str, news_list))\n",
        "news_str\n",
        "\n",
        "spacy(news_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gz7-0U41-y3m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sentiment3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1nYQc3+Q/kVf0mSHvB1+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}